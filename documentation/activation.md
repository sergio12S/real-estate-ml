# Функции активации

Отвечает за:
1. Насколько хорошо модель может объяснить набор данных
2. Определяет тип прогнозов, поэтому очень важно определять функцию активацию на выходном слое
3. Позволяют решить проблему "затухающего градиента"

По умолчанию ставят функцию активации Relu
Функции активацию используются внутри нейронной сети и на ее выходе.

Best practise:
1. Внутри нейронной сети использовать ту же самую функцию активации

Функции активации (кроме линейной) придают данным нелинейность. Следовательно нейронная сеть может лучше понять не линейные закономерности и тем самым оптимально расспределить свои веса, чтобы минимизировать функцию потерь. 

Основные функции активации:
1. Relu ( выпрямленная линейная функция)
2. Sigmoid (логистическая функция)
3. Tanh тангенс

# Relu
Relu - самая расспространенная функция, которая используеться длы скрытых слоев. Менее воспреимчив к проблеме исезающих градиентов. Но в то же время, приводит к перенасыщению или убивает нейроны сети. Поэтому важно использовать регулизацию l1, l2.
Обычно используют инициализацию весов He Normal, He Uniform. А также стандартизируют данные от 0 до 1

from matplotlib import pyplot

def rectified(x):
	return max(0.0, x)

inputs = [x for x in range(-10, 10)]
outputs = [rectified(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()

# Sigmoid
Используется в задачах логистической регрессии. Бинарная класификация.
Выводит значение от 0 до 1. Чем ближе к 1, тем более вероятен True , чем ближе к 0, тем более вероятен False. 

from math import exp
from matplotlib import pyplot
def sigmoid(x):
	return 1.0 / (1.0 + exp(-x))
inputs = [x for x in range(-10, 10)]
outputs = [sigmoid(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()

Рекомендации:
1. Иниициализация веса Xavier Normal или Xavier Uniform (Glorot)
2. Мастабировать данные от 0 до 1

# Tanh
Функция принимает значение в качестве входа и выводит значения в диапазоне от -1 до 1. Чем больше вход (более положительный), тем ближе выходное значение к 1,0, тогда как чем меньше вход (более отрицательный), тем ближе выход будет -1.0.

from math import exp
from matplotlib import pyplot
 
def tanh(x):
	return (exp(x) - exp(-x)) / (exp(x) + exp(-x))
 
inputs = [x for x in range(-10, 10)]
outputs = [tanh(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()
1. Иниициализация веса Xavier Normal или Xavier Uniform (Glorot)
2. Мастабировать данные от -1 до 1

Эта функция приводит к смерти нейронов. 


# Какие проблемы?
Tang, sigmoid - провоцируют проблемы исчезающего градиента. Это случается тогда, когда значение около 0 перемножаються, то они стремятся к нулю и в итоге этот градиент не улавливает закономерности в данных. Тоже самое и с обратной стороны, значения около границ могут применять слишком экстримальные значения и поэтому нужно штрафовать через регулизацию.  

# Какая функция активации для скрытых слоев?
Обычно нейронные сети MLP, CNN исползуют Relu
Рекурентные сти используют Tanh, Sigmoid

# Какая функция активации для выходных слоев?

# Linear Линейная активация
from matplotlib import pyplot
 
def linear(x):
	return x
inputs = [x for x in range(-10, 10)]
outputs = [linear(x) for x in inputs]
pyplot.plot(inputs, outputs)
pyplot.show()

Как видно из кода, эта функция ничего не делает. Поэтому ее еще называют "без активации". Она по умолчанию стоит для активации выходного слоя в keras. 
С помощью этой функции активации решат проблемы регрессии.
Если использовать эту функции активации для решения проблем регрессии, то данные необходимо нормализировать или стандартизировать.

# Softmax
Выводит вектор значений, сумма которых ровняеться 1. Это интерпритируеться как сумма вероятностей классов. Поэтому она используеться для решения проблем мультиклассовая классификации. 
def softmax(x):
	return exp(x) / exp(x).sum()
 
inputs = [1.0, 3.0, 2.0]
outputs = softmax(inputs)
print(outputs)
print(outputs.sum())

Регрессия
Для решения проблем регресси - линейная активация

Классификация

Бинарная классификация - сигмоид
Мультиклассовая классификация - софтмакс
Мультилейбел - Сигмоид